{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the datasets\n",
    "\n",
    "Files are originally in .dta files for use in Stata. Using the pandas library, they have been converted into .csv files for use in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Converting"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_stata(\"Wave1/Adult_W1_Anon_V7.0.0.dta\")\n",
    "data.to_csv(\"wave1.csv\", index=False)\n",
    "\n",
    "data = pd.read_stata(\"Wave2/Adult_W2_Anon_V4.0.0.dta\")\n",
    "data.to_csv(\"wave2.csv\", index=False)\n",
    "\n",
    "data = pd.read_stata(\"Wave3/Adult_W3_Anon_V3.0.0.dta\")\n",
    "data.to_csv(\"wave3.csv\", index=False)\n",
    "\n",
    "data = pd.read_stata(\"Wave4/Adult_W4_Anon_V2.0.0.dta\")\n",
    "data.to_csv(\"wave4.csv\", index=False)\n",
    "\n",
    "data = pd.read_stata(\"Wave5/Adult_W5_Anon_V1.0.0.dta\")\n",
    "data.to_csv(\"wave5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Each wave must be preprocessed. This will include:\n",
    "- Assigning labels according to the CESD-10 reporting scale\n",
    "- Discretizing/classing continuous variables\n",
    "- Feature Selection/Engineering\n",
    "- Normalisation/Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_254227/2745299220.py:6: DtypeWarning: Columns (11,14,18,30,38,77,133,134,135,137,138,139,140,141,142,143,145,146,147,148,149,150,151,153,154,155,156,157,158,159,161,162,163,164,165,166,167,169,170,171,172,175,177,178,179,180,183,219,223,235,236,237,238,239,240,241,242,247,251,254,261,264,265,267,269,271,277,283,291,292,293,294,295,296,297,298,299,301,309,311,313,315,317,319,321,323,325,327,329,331,333,337,339,341,343,345,347,350,358,362,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,407,408,417,418,419,427,428,429,440,441,442,443,447,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,470,471,472,477,478,479,481,482,483,490,491,492,493,510,512,516,518,522,523,525,528,529,531,532,534,535,537,538,540,549,552,553,564,567,569,571,572,573,576,578,580,590,591,594,599,601,641,654,658,660,662,668,671,672,693,731,789,790,791,792,793,800) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"wave1.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"wave1.csv\")\n",
    "\n",
    "# Selecting column names for CESD-10 Scale related features\n",
    "cesd_col_names = [\"_a_emobth\", \"_a_emomnd\", \"_a_emodep\", \"_a_emoeff\", \"_a_emohope\",\n",
    "                \"_a_emofear\", \"_a_emoslp\", \"_a_emohap\", \"_a_emolone\", \"_a_emogo\"]\n",
    "\n",
    "desc_col_names = [\"w1_a_gen\", \"w1_a_dob_y\", \"w1_a_popgrp\", \"w1_a_marstt\", \"w1_a_mthali\", \"w1_a_em1\", \"w1_a_brnprov\",\n",
    "                  \"w1_a_em1hrs\", \"w1_a_ems\", \"w1_a_emshrs\", \"w1_a_owncom\", \"w1_a_owncel\", \"w1_a_edschgrd\",\n",
    "                  \"w1_a_edter\", \"w1_a_ed07sub\", \"w1_a_ed08cur\", \"w1_a_ed08cursub\", \"w1_a_hldes\", \"w1_a_hlcon\",\n",
    "                  ]\n",
    "\n",
    "new_df = pd.DataFrame({})\n",
    "\n",
    "# Rename specific columns\n",
    "df = df.rename(columns={'w1_a_gen': 'gender',\n",
    "                        'w1_a_dob_y': 'date_of_birth',\n",
    "                        'w1_a_popgrp': 'race',\n",
    "                        'w1_a_marstt': 'marital_status',\n",
    "                        'w1_a_brnprov': 'living_province',\n",
    "                        'w1_a_mthali': 'parents_alive',\n",
    "                        'w1_a_em1': 'employed',\n",
    "                        'w1_a_edschgrd': 'highest_grade_school',\n",
    "                        'w1_a_edter': 'tertiary_education',\n",
    "                        'w1_a_ed07att': 'attended_courses',\n",
    "                        'w1_a_ed08cur': 'currently_enrolled'})\n",
    "\n",
    "# Map the values to integers\n",
    "df['gender'] = df['gender'].map({'Female': 0, 'Male': 1}).fillna(-1).astype(int)\n",
    "new_df['gender'] = df['gender']\n",
    "\n",
    "# Factorize the column, getting the integer codes and the unique values\n",
    "# Replace -1 (which is the default code for NaN in factorize) with any value you want (e.g., -1)\n",
    "df['race'] = df['race'].replace(pd.NA, 'Missing')\n",
    "df['race'], unique_values = pd.factorize(df['race'])\n",
    "new_df['race'] = df['race']\n",
    "\n",
    "# Factorize the column, getting the integer codes and the unique values\n",
    "df['marital_status'] = df['marital_status'].replace(pd.NA, 'Missing')\n",
    "df['marital_status'], unique_values = pd.factorize(df['marital_status'])\n",
    "new_df['marital_status'] = df['marital_status']\n",
    "\n",
    "# Factorize the column, getting the integer codes and the unique values\n",
    "df['living_province'] = df['living_province'].replace(pd.NA, 'Missing')\n",
    "df['living_province'], unique_values = pd.factorize(df['living_province'])\n",
    "new_df['living_province'] = df['living_province']\n",
    "\n",
    "# Factorize the column, getting the integer codes and the unique values\n",
    "df['parents_alive'] = df['parents_alive'].replace(pd.NA, 'Missing')\n",
    "df['parents_alive'], unique_values = pd.factorize(df['parents_alive'])\n",
    "new_df['parents_alive'] = df['parents_alive']\n",
    "\n",
    "# Factorize the column, getting the integer codes and the unique values\n",
    "# df['highest_grade_school'] = df['highest_grade_school'].replace(pd.NA, 'Missing')\n",
    "# df['highest_grade_school'] = df['highest_grade_school'].replace('Not Applicable', 'Missing')\n",
    "df['highest_grade_school'], unique_values = pd.factorize(df['highest_grade_school'])\n",
    "new_df['highest_grade_school'] = df['highest_grade_school']\n",
    "\n",
    "df['tertiary_education'], unique_values = pd.factorize(df['tertiary_education'])\n",
    "new_df['tertiary_education'] = df['tertiary_education']\n",
    "\n",
    "df['attended_courses'], unique_values = pd.factorize(df['attended_courses'])\n",
    "new_df['attended_courses'] = df['attended_courses']\n",
    "\n",
    "df['currently_enrolled'], unique_values = pd.factorize(df['currently_enrolled'])\n",
    "new_df['currently_enrolled'] = df['currently_enrolled']\n",
    "\n",
    "for i in cesd_col_names:\n",
    "    new_df[f\"w1{i}\"] = df[f\"w1{i}\"]\n",
    "\n",
    "new_df['w1_a_outcome'] = df['w1_a_outcome']\n",
    "new_df['pid'] = df['pid']\n",
    "\n",
    "# Assuming 'df' is your pandas DataFrame\n",
    "new_df.to_csv('wave1_select.csv', index=False)\n",
    "\n",
    "# print(df[\"highest_grade_school\"].unique())\n",
    "\n",
    "# Factorize the column, getting the integer codes and the unique values\n",
    "# df['employed'] = df['employed'].replace(pd.NA, 'Missing')\n",
    "# df['employed'], unique_values = pd.factorize(df['employed'])\n",
    "# new_df['employed'] = df['employed']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Each of the 5 waves will be converted into pandas dataframes. Waves 1, 3 and 5 will be used as training data. Waves 2 and 4 will be validation and testing data respectively.\n",
    "\n",
    "Feature selection has been done as follows:\n",
    "- Features relating to the CESD-10 reporting scale are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wave1_select.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "'''\n",
    "Creating a class for each wave.\n",
    "'''\n",
    "\n",
    "class Wave:\n",
    "    def __init__(self, data: pd.DataFrame, select_cols: list):\n",
    "        self.data : pd.DataFrame = data\n",
    "        self.set_select_cols : List[str] = select_cols\n",
    "\n",
    "    def set_select_cols(self, select_cols: list):\n",
    "        self.set_select_cols = select_cols\n",
    "    \n",
    "    def get_select_cols(self):\n",
    "        return self.set_select_cols\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data)\n",
    "\n",
    "waves: List[Wave] = []\n",
    "\n",
    "# Selecting column names for CESD-10 Scale related features\n",
    "cesd_col_names = [\"_a_emobth\", \"_a_emomnd\", \"_a_emodep\", \"_a_emoeff\", \"_a_emohope\",\n",
    "                \"_a_emofear\", \"_a_emoslp\", \"_a_emohap\", \"_a_emolone\", \"_a_emogo\"]\n",
    "\n",
    "desc_col_names = [\"w1_a_gen\", \"w1_a_dob_y\", \"w1_a_popgrp\", \"w1_a_marstt\", \"w1_a_mthali\", \"w1_a_em1\",\n",
    "                  \"w1_a_em1hrs\", \"w1_a_ems\", \"w1_a_emshrs\", \"w1_a_owncom\", \"w1_a_owncel\", \"w1_a_edschgrd\",\n",
    "                  \"w1_a_edter\", \"w1_a_ed07sub\", \"w1_a_ed08cur\", \"w1_a_ed08cursub\", \"w1_a_hldes\", \"w1_a_hlcon\",\n",
    "                  ]\n",
    "\n",
    "'''\n",
    "Running through each wave\n",
    "'''\n",
    "for i in range(1, 2):\n",
    "    url = 'wave' + str(i) + '_select.csv'\n",
    "    print(url)\n",
    "    data = pd.read_csv(url)\n",
    "\n",
    "    # Header text for each column based on wave\n",
    "    header = 'w' + str(i)\n",
    "    \n",
    "    select_cols = []\n",
    "    for i in cesd_col_names:\n",
    "        select_cols.append(header + i)\n",
    "\n",
    "    # Drop rows where Interview Outcome is 'Refused/Not Available'\n",
    "    outcome_str = header + '_a_outcome'\n",
    "    new_data = data[data[outcome_str] == 'Successfully Interviewed']\n",
    "\n",
    "    '''\n",
    "    In some cases, despite accepting the interview, there are 3 cases leading to incomplete data:\n",
    "        1. Some participants might have refused certain questions (Refused)\n",
    "        2. Some participants might not have placed answers on the survey (Missing)\n",
    "        3. Some participants outlined that they did not have an answer that aligned with any of the options (Don't know)\n",
    "\n",
    "    These participants will be dropped from the dataset\n",
    "    '''\n",
    "\n",
    "    cesd_valid_answers = ['Rarely or none of the time (less than 1 day)',\n",
    "                          'Some or little of the time (1-2 days)',\n",
    "                          'Occasionally or a moderate amount of time (3-4 days)',\n",
    "                          'All of the time (5-7 days)']\n",
    "    count = 1\n",
    "    for i in select_cols:\n",
    "        # new_data = new_data[new_data[i] != 'Missing']\n",
    "        # new_data = new_data[new_data[i] != 'Refused']\n",
    "        # new_data = new_data[new_data[i] != \"Don't know\"]\n",
    "        new_data = new_data[new_data[i].isin(cesd_valid_answers)]\n",
    "        new_data.dropna()\n",
    "\n",
    "        # Questions 5 and 8 have reverse scoring\n",
    "        if (count == 5 or count == 8):\n",
    "            new_data[i] = new_data[i].replace('Rarely or none of the time (less than 1 day)', 3)\n",
    "            new_data[i] = new_data[i].replace('Some or little of the time (1-2 days)', 2)\n",
    "            new_data[i] = new_data[i].replace('Occasionally or a moderate amount of time (3-4 days)', 1)\n",
    "            new_data[i] = new_data[i].replace('All of the time (5-7 days)', 0)\n",
    "        else:\n",
    "            new_data[i] = new_data[i].replace('Rarely or none of the time (less than 1 day)', 0)\n",
    "            new_data[i] = new_data[i].replace('Some or little of the time (1-2 days)', 1)\n",
    "            new_data[i] = new_data[i].replace('Occasionally or a moderate amount of time (3-4 days)', 2)\n",
    "            new_data[i] = new_data[i].replace('All of the time (5-7 days)', 3)\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    wave = Wave(new_data, select_cols)\n",
    "    waves.append(wave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling\n",
    "\n",
    "Each participant was **labelled according to the CESD-10 reporting scale**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labelling Participants: 100%|██████████| 15342/15342 [00:02<00:00, 5285.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5185\n",
      "Total Depressed Participants: 5185\n",
      "Calculated Prevalence for Depression: 33.8%\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Selecting column names for CESD-10 Scale related features\n",
    "cesd_col_names = [\"_a_emobth\", \"_a_emomnd\", \"_a_emodep\", \"_a_emoeff\", \"_a_emohope\",\n",
    "                \"_a_emofear\", \"_a_emoslp\", \"_a_emohap\", \"_a_emolone\", \"_a_emogo\"]\n",
    "\n",
    "for wave in range(len(waves)):\n",
    "    print(f\"Wave {wave+1}:\")\n",
    "    data: pd.DataFrame = waves[wave].data\n",
    "    select_cols = waves[wave].get_select_cols()\n",
    "\n",
    "    # Dictionary to store the scores and depression status\n",
    "    scores: Dict[str, Dict] = dict()\n",
    "\n",
    "    # Series containing participant IDs\n",
    "    participants = data['pid']\n",
    "\n",
    "    # Counter for the number of participants flagged as depressed\n",
    "    count_depressed = 0\n",
    "\n",
    "    # Iterate over each participant\n",
    "    for participant in tqdm(participants, desc=\"Labelling Participants\"):\n",
    "        score = 0\n",
    "        depressed = False\n",
    "\n",
    "        idx = data.index[data['pid'] == participant]\n",
    "\n",
    "        # Sum the scores for all relevant columns\n",
    "        for col in select_cols:\n",
    "            value = data.at[idx[0], col]  # Accessing the value using participant ID and column name\n",
    "            score += value\n",
    "\n",
    "        # Determine if the participant is depressed based on the score\n",
    "        if score >= 10:\n",
    "            depressed = True\n",
    "            count_depressed += 1\n",
    "\n",
    "        # Map the participant ID to their score and depression status\n",
    "        scores[participant] = {'score': int(score), 'Depressed': depressed}\n",
    "    \n",
    "    data['depression_score'] = 0\n",
    "\n",
    "    for i in cesd_col_names:\n",
    "        data['depression_score'] += data[f\"w{wave+1}{i}\"]\n",
    "    \n",
    "    # Create a new column 'new_column' based on a condition\n",
    "    data['depressed'] = data['depression_score'].apply(lambda x: 1 if x >= 10 else 0)\n",
    "\n",
    "    print(list(data['depressed']).count(1))\n",
    "\n",
    "    data.to_csv('wave1_select_labelled.csv')\n",
    "    # Print the total number of depressed participants\n",
    "    print(f\"Total Depressed Participants: {count_depressed}\")\n",
    "    print(f\"Calculated Prevalence for Depression: {round(count_depressed/len(participants) * 100, 2)}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current models being implemented are namely:\n",
    "\n",
    "* Supervised\n",
    "    1. Logistic Regression Model\n",
    "    2. Random Forest Classifier\n",
    "    3. Support Vector Machine\n",
    "    4. Deep Neural Network\n",
    "    5. (New) Bayesian Network\n",
    "    6. (New) Gradient Boosting\n",
    "\n",
    "* Unsupervised\n",
    "    1. (New) K-Means Clustering\n",
    "    2. (New) Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/useradd/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 1.00\n",
      "Validation Confusion Matrix:\n",
      "[[2020    2]\n",
      " [   4 1043]]\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2022\n",
      "           1       1.00      1.00      1.00      1047\n",
      "\n",
      "    accuracy                           1.00      3069\n",
      "   macro avg       1.00      1.00      1.00      3069\n",
      "weighted avg       1.00      1.00      1.00      3069\n",
      "\n",
      "Test Accuracy: 1.00\n",
      "Test Confusion Matrix:\n",
      "[[2030    2]\n",
      " [   5 1032]]\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2032\n",
      "           1       1.00      1.00      1.00      1037\n",
      "\n",
      "    accuracy                           1.00      3069\n",
      "   macro avg       1.00      1.00      1.00      3069\n",
      "weighted avg       1.00      1.00      1.00      3069\n",
      "\n",
      "Best Parameters: {'logisticregression__C': 0.1, 'logisticregression__penalty': 'l1'}\n",
      "Best Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('wave1_select_labelled.csv')\n",
    "\n",
    "# First, separate the target variable (label) from the features\n",
    "# Assuming you have a target column in your original dataframe called 'depression_label' that indicates depression status\n",
    "# If not, make sure to create that before splitting\n",
    "y = df['depressed']  # Replace with your actual target column name\n",
    "X = df.drop(columns=['depressed', 'w1_a_outcome', 'pid'])  # The features you've created\n",
    "\n",
    "# Now split the data into train+validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now split the train+validation set into actual train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "print(len(X_train))\n",
    "\n",
    "# This gives you 60% train, 20% validation, and 20% test splits\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Train the model on the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = log_reg.predict(X_val)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "print(val_conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "val_class_report = classification_report(y_val, y_val_pred)\n",
    "print(\"Validation Classification Report:\")\n",
    "print(val_class_report)\n",
    "\n",
    "# Accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(test_conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "test_class_report = classification_report(y_test, y_test_pred)\n",
    "print(\"Test Classification Report:\")\n",
    "print(test_class_report)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    " \n",
    "# Create a pipeline with scaler and logistic regression\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000, solver='saga', tol=0.1))\n",
    " \n",
    "# Create a parameter grid\n",
    "param_grid = {\n",
    "    'logisticregression__C': [0.1, 1, 10, 100],\n",
    "    'logisticregression__penalty': ['l1', 'l2']\n",
    "}\n",
    " \n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5)\n",
    " \n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    " \n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
